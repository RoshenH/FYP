{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7e28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and filter (same as original)\n",
    "X_flux = np.load(\"X_flux_512.npy\")[..., np.newaxis]  # (samples, 512, 1)\n",
    "X_tabular = np.load(\"X_tabular.npy\")\n",
    "y = np.load(\"y.npy\")\n",
    "mask = y != -1\n",
    "X_flux, X_tabular, y = X_flux[mask], X_tabular[mask], y[mask]\n",
    "\n",
    "# Train/Val split\n",
    "Xf_train, Xf_val, Xt_train, Xt_val, y_train, y_val = train_test_split(\n",
    "    X_flux, X_tabular, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize tabular data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "Xt_train = scaler.fit_transform(Xt_train)\n",
    "Xt_val = scaler.transform(Xt_val)\n",
    "\n",
    "# Apply SMOTE to tabular data\n",
    "smote = SMOTE(random_state=42)\n",
    "Xt_train_smote, y_train_smote = smote.fit_resample(Xt_train, y_train)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Focal Loss implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "# Compute class weights for neural networks\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92706c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¸ Logistic Regression (Tabular Only) with SMOTE\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "False Positive       0.95      0.65      0.78       127\n",
      "     Confirmed       0.44      0.90      0.59        39\n",
      "\n",
      "      accuracy                           0.71       166\n",
      "     macro avg       0.70      0.78      0.68       166\n",
      "  weighted avg       0.83      0.71      0.73       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Logistic Regression (Tabular Only) with SMOTE\n",
    "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "clf.fit(Xt_train_smote, y_train_smote)\n",
    "log_preds = clf.predict(Xt_val)\n",
    "\n",
    "print(\"\\nðŸ”¸ Logistic Regression (Tabular Only) with SMOTE\")\n",
    "log_report = classification_report(y_val, log_preds, target_names=[\"False Positive\", \"Confirmed\"], output_dict=True)\n",
    "print(classification_report(y_val, log_preds, target_names=[\"False Positive\", \"Confirmed\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43ff6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¸ MLP (Tabular Only) with Focal Loss\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "False Positive       0.99      0.85      0.92       127\n",
      "     Confirmed       0.67      0.97      0.79        39\n",
      "\n",
      "      accuracy                           0.88       166\n",
      "     macro avg       0.83      0.91      0.85       166\n",
      "  weighted avg       0.91      0.88      0.89       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. MLP (Tabular Only) with Class Weighting and Focal Loss\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(5, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "tabular_model = TabularMLP().to(device)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "optimizer = torch.optim.Adam(tabular_model.parameters(), lr=1e-3)\n",
    "\n",
    "Xtt_train = torch.tensor(Xt_train_smote, dtype=torch.float32).to(device)\n",
    "ytt_train = torch.tensor(y_train_smote, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "for epoch in range(100):\n",
    "    tabular_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = tabular_model(Xtt_train)\n",
    "    loss = criterion(output, ytt_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss for early stopping\n",
    "    tabular_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = tabular_model(torch.tensor(Xt_val, dtype=torch.float32).to(device))\n",
    "        val_loss = criterion(val_output, torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device))\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "        break\n",
    "\n",
    "tabular_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_logits = tabular_model(torch.tensor(Xt_val, dtype=torch.float32).to(device))\n",
    "    val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "    val_preds = (val_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nðŸ”¸ MLP (Tabular Only) with Focal Loss\")\n",
    "mlp_report = classification_report(y_val, val_preds, target_names=[\"False Positive\", \"Confirmed\"], output_dict=True)\n",
    "print(classification_report(y_val, val_preds, target_names=[\"False Positive\", \"Confirmed\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e721468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¸ 1D CNN (Flux Only) with Focal Loss\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "False Positive       0.77      1.00      0.87       127\n",
      "     Confirmed       0.00      0.00      0.00        39\n",
      "\n",
      "      accuracy                           0.77       166\n",
      "     macro avg       0.38      0.50      0.43       166\n",
      "  weighted avg       0.59      0.77      0.66       166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. 1D CNN (Flux Only) with Class Weighting and Focal Loss\n",
    "class FluxCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "flux_model = FluxCNN().to(device)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "optimizer = torch.optim.Adam(flux_model.parameters(), lr=1e-3)\n",
    "\n",
    "Xff_train = torch.tensor(Xf_train, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
    "yff_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "for epoch in range(100):\n",
    "    flux_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = flux_model(Xff_train)\n",
    "    loss = criterion(output, yff_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss for early stopping\n",
    "    flux_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = flux_model(torch.tensor(Xf_val, dtype=torch.float32).permute(0, 2, 1).to(device))\n",
    "        val_loss = criterion(val_output, torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device))\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "        break\n",
    "\n",
    "flux_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_flux_tensor = torch.tensor(Xf_val, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
    "    val_logits = flux_model(val_flux_tensor)\n",
    "    val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "    val_preds = (val_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nðŸ”¸ 1D CNN (Flux Only) with Focal Loss\")\n",
    "cnn_report = classification_report(y_val, val_preds, target_names=[\"False Positive\", \"Confirmed\"], output_dict=True)\n",
    "print(classification_report(y_val, val_preds, target_names=[\"False Positive\", \"Confirmed\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f555a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¸ Enhanced Early Fusion (CNN + MLP) with Focal Loss\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "False Positive       0.77      1.00      0.87       127\n",
      "     Confirmed       0.00      0.00      0.00        39\n",
      "\n",
      "      accuracy                           0.77       166\n",
      "     macro avg       0.38      0.50      0.43       166\n",
      "  weighted avg       0.59      0.77      0.66       166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Roshen Hasangha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Enhanced Early Fusion (CNN + MLP) with Class Weighting and Focal Loss\n",
    "class EnhancedEarlyFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flux_cnn = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(64 + 5, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, flux, tabular):\n",
    "        flux_features = self.flux_cnn(flux)\n",
    "        combined = torch.cat((flux_features, tabular), dim=1)\n",
    "        return self.mlp(combined)\n",
    "\n",
    "fusion_model = EnhancedEarlyFusion().to(device)\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=1e-3)\n",
    "\n",
    "Xff_train = torch.tensor(Xf_train, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
    "Xtt_train = torch.tensor(Xt_train, dtype=torch.float32).to(device)\n",
    "yff_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "for epoch in range(100):\n",
    "    fusion_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = fusion_model(Xff_train, Xtt_train)\n",
    "    loss = criterion(output, yff_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss for early stopping\n",
    "    fusion_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = fusion_model(\n",
    "            torch.tensor(Xf_val, dtype=torch.float32).permute(0, 2, 1).to(device),\n",
    "            torch.tensor(Xt_val, dtype=torch.float32).to(device)\n",
    "        )\n",
    "        val_loss = criterion(val_output, torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device))\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= patience:\n",
    "        break\n",
    "\n",
    "fusion_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_logits = fusion_model(\n",
    "        torch.tensor(Xf_val, dtype=torch.float32).permute(0, 2, 1).to(device),\n",
    "        torch.tensor(Xt_val, dtype=torch.float32).to(device)\n",
    "    )\n",
    "    val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "    val_preds = (val_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nðŸ”¸ Enhanced Early Fusion (CNN + MLP) with Focal Loss\")\n",
    "fusion_report = classification_report(y_val, val_preds, target_names=[\"False Positive\", \"Confirmed\"], output_dict=True)\n",
    "print(classification_report(y_val, val_preds, target_names=[\"False Positive\", \"Confirmed\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
